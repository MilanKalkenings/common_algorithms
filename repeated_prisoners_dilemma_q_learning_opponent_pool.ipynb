{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0144491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Callable\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "def score(player_a: bool, player_b: bool) -> Tuple[int, int]:\n",
    "    if player_a and player_b:\n",
    "        return (3, 3)\n",
    "    if player_a and not player_b:\n",
    "        return (0, 5)\n",
    "    if not player_a and player_b:\n",
    "        return (5, 0)\n",
    "    else:\n",
    "        return (1, 1)\n",
    "    \n",
    "\n",
    "def repeated_prisoners_dilemma(n_repetitions: int, strategy_player_a: Callable, strategy_player_b: Callable) -> Tuple[Tuple[int, int], Tuple[List[bool], List[bool]]]:\n",
    "    score_a = 0\n",
    "    score_b = 0\n",
    "    history_player_a = []\n",
    "    history_player_b = []\n",
    "    for _ in range(n_repetitions):\n",
    "        turn_player_a = strategy_player_a(history_this_player=history_player_a, history_other_player=history_player_b)\n",
    "        turn_player_b = strategy_player_b(history_this_player=history_player_b, history_other_player=history_player_a)\n",
    "        turn_scores = score(player_a=turn_player_a, player_b=turn_player_b)\n",
    "        score_a += turn_scores[0]\n",
    "        score_b += turn_scores[1]\n",
    "        history_player_a.append(turn_player_a)\n",
    "        history_player_b.append(turn_player_b)\n",
    "    return ((score_a, score_b), (history_player_a, history_player_b))\n",
    "\n",
    "\n",
    "\n",
    "class QModel:\n",
    "    \"\"\"\n",
    "    updates using Bellmann Equation:\n",
    "    Q(s,a) <- Q(s,a) + learning_rate * (immediate_reward + discount_factor * max_a'Q(s',a') - Q(s,a)) \n",
    "    \n",
    "    Q(s,a) are initialized as 0\n",
    "    \n",
    "    True: player cooperates\n",
    "    False: player defects\n",
    "    \"\"\"\n",
    "    def __init__(self, exploration_rate: float = 0.2, learning_rate: float = 0.05, discount_factor: float = 0.99, max_history: int = 5, reward_function: str = \"raw\", verbose: bool = False):\n",
    "        \"\"\"\n",
    "        exploration_rate: ranges from 0 to 1\n",
    "            0: always take the action that lead to best Q(s,a) in history (exploit). risk: get stuck in suboptimal behavior\n",
    "            ->1: deviate from the historically best action for better exploration \n",
    "\n",
    "        learning rate: ranges from 0 to 1\n",
    "            1: completely replace current Q(s,a)\n",
    "            ->0: barely any update\n",
    "\n",
    "        discount_factor: ranges from 0 to 1\n",
    "            0: mypotic (only immediate reward matters)\n",
    "            ->1: long term rewards count up to as much as the immediate reward\n",
    "\n",
    "        max_history:\n",
    "            history that is considered by the model. \n",
    "            if less than max_history turns happened, the state only contains the so far performed turns.\n",
    "\n",
    "        verbose:\n",
    "            True: print out information \n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.max_history = max_history\n",
    "        self.q = self.init_q()\n",
    "        self.training_mode = True\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.reward_function = reward_function\n",
    "\n",
    "\n",
    "    def reward(self, action_this_player: bool, action_other_player: bool) -> int:\n",
    "        if self.reward_function == \"raw\":\n",
    "            return score(player_a=action_this_player, player_b=action_other_player)[0]\n",
    "        elif self.reward_function == \"win_or_lose\":\n",
    "            \"\"\"\n",
    "            like in win-stay lose-shift:\n",
    "            CC (Reward, good for both) or DC (Temptation, best for this player) -> stay \n",
    "            CD (Sucker, worst for this player) or DD (Punishment, bad for both) -> shift\n",
    "            \"\"\"\n",
    "            if action_other_player:  # CC or DC\n",
    "                return 1\n",
    "            else:  # CD or DD\n",
    "                return 0\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{self.reward_function} is not a valid reward_funciton. Valid options: 'raw', 'win_or_lose'\" )\n",
    "\n",
    "\n",
    "    def init_q(self):\n",
    "        possible_states = []\n",
    "        for length in range(1, (self.max_history + 1) * 2):\n",
    "            if length % 2 == 0:\n",
    "                for combo in itertools.product([0, 1], repeat=length):\n",
    "                    possible_states.append(str(np.array(combo, dtype=int)))\n",
    "        q = {\"init\": {False: 0., True: 0.}}\n",
    "        for s in possible_states:\n",
    "            q[s] = {False: 0., True: 0.}\n",
    "        return q\n",
    "\n",
    "\n",
    "    def select_action(self, state: str):\n",
    "        # explore randomly selecting next state \n",
    "        if self.training_mode:\n",
    "            if random.random() <= self.exploration_rate:\n",
    "                return random.random() <= 0.5\n",
    "        # exploit state knowledge\n",
    "        selected_action = self.q[state][True] >= self.q[state][False] \n",
    "        if self.verbose:\n",
    "            print(\"select action\")\n",
    "            print(f\"Q({state}, False): {self.q[state][False]}\")\n",
    "            print(f\"Q({state}, True): {self.q[state][True]}\")\n",
    "            print(f\"so: {self.q[state][True] >= self.q[state][False]}\\n\")\n",
    "        return selected_action\n",
    "    \n",
    "\n",
    "    def step(self, history_this_player: List[bool], history_other_player: List[bool]) -> bool:\n",
    "        \"\"\"\n",
    "        select next action and update Q for last state and last action if self.training_mode = True\n",
    "        \"\"\"\n",
    "        # 1. define state\n",
    "        if len(history_this_player) == 0:\n",
    "            state = \"init\"\n",
    "            if self.verbose: print(f\"state = {state}\")\n",
    "        else:\n",
    "            last_actions_this_player = [int(i) for i in history_this_player[-self.max_history:][::-1]]\n",
    "            last_actions_other_player = [int(i) for i in history_other_player[-self.max_history:]]\n",
    "            state_np = np.concat([last_actions_this_player, last_actions_other_player])\n",
    "            state = str(state_np)\n",
    "            if self.verbose: print(f\"state = {state}\")\n",
    "\n",
    "            # 2. (optional) update q\n",
    "            if self.training_mode:\n",
    "                last_action_this_player = history_this_player[-1]\n",
    "                last_action_other_player = history_other_player[-1]\n",
    "                immediate_reward = self.reward(action_this_player=last_action_this_player, action_other_player=last_action_other_player)\n",
    "                if len(history_this_player) == 1:\n",
    "                    last_state = \"init\"\n",
    "                else:\n",
    "                    last_state_np = np.concat([[int(i) for i in history_this_player[-(self.max_history+1):][::-1]], [int(i) for i in history_other_player[-(self.max_history+1):]]])\n",
    "                    last_state = str(last_state_np[1:-1])\n",
    "                if self.verbose: print(f\"last state = {last_state}\")\n",
    "                max_next_q = max(self.q[state][True], self.q[state][False])\n",
    "                current_q = self.q[last_state][last_action_this_player]\n",
    "                # update using bellmann equation\n",
    "                self.q[last_state][last_action_this_player] = current_q + self.learning_rate * (immediate_reward + self.discount_factor * max_next_q - current_q)\n",
    "                if self.verbose:\n",
    "                    print(f\"update Q(s,a) = Q({last_state},{last_action_this_player})\")\n",
    "                    print(f\"Q(s,a): {current_q}\")\n",
    "                    print(f\"Q(s',a')_max_a': {max_next_q}\")\n",
    "                    print(f\"Q(s,a) <- {current_q} + {self.learning_rate} * ({immediate_reward} + {self.discount_factor} * {max_next_q} - {current_q}) = {current_q + self.learning_rate * (immediate_reward + self.discount_factor * max_next_q - current_q)}\")\n",
    "\n",
    "        # 3. perform next action\n",
    "        if state in self.q.keys():\n",
    "            return self.select_action(state=state)\n",
    "        else:\n",
    "            return random.random() > 0.2\n",
    "        \n",
    "        \n",
    "    def info(self):\n",
    "        select_c = 0\n",
    "        select_d = 0\n",
    "        select_c_due_to_unseen = 0\n",
    "        for state in self.q.keys():\n",
    "            if self.q[state][True] == self.q[state][False] == 0:\n",
    "                select_c_due_to_unseen += 1\n",
    "            elif self.q[state][True] >= self.q[state][False]:\n",
    "                select_c += 1\n",
    "            else:\n",
    "                select_d += 1\n",
    "        print(f\"select c: {select_c}, select d: {select_d}, unseen {select_c_due_to_unseen}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NonparametricStrategies:\n",
    "    \"\"\"\n",
    "    True: player cooperates\n",
    "    False: player defects\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def strategy_always_cooperate(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_always_defect(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_tit_for_tat(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        start with cooperation and afterwards exactly mirrors the last turn of the opponent\n",
    "        \"\"\"\n",
    "        if len(history_this_player) == 0:\n",
    "            return True\n",
    "        return history_other_player[-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def strategy_tit_for_tat_generous(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        tit for tat but with 10% chance forgives deflection of other player in last turn\n",
    "        \"\"\"\n",
    "        if len(history_this_player) == 0:\n",
    "            return True\n",
    "        regular_turn = history_other_player[-1] \n",
    "        if regular_turn == False:\n",
    "            return random.random() > 0.9\n",
    "        return regular_turn\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_tit_for_tat_suspicious(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        tit for tat but starting with deflection (prefered in hostile environments)\n",
    "        \"\"\"\n",
    "        if len(history_this_player) == 0:\n",
    "            return False\n",
    "        return history_other_player[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_tit_for_tat_noisy(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        randomly deviates from regular tit for tat turn with 10% chance \n",
    "        \"\"\"\n",
    "        if len(history_this_player) == 0:\n",
    "            regular_turn = True\n",
    "        else:\n",
    "            regular_turn = history_other_player[-1]\n",
    "\n",
    "        if random.random() > 0.9:\n",
    "            return not regular_turn\n",
    "        return regular_turn\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_tit_for_tat_exponential_decay(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        tit for tat but considering the history of the opponents actions, assigning more weight to recent moves.\n",
    "        The player cooperates with a probability proportional to the weighted fraction of the opponents cooperation.\n",
    "        \"\"\"\n",
    "        if len(history_this_player) == 0:\n",
    "            return True\n",
    "        weights = np.logspace(start=0.1, stop=1, base=10, num=len(history_other_player))\n",
    "        return bool(random.random() < (weights[history_other_player].sum() / weights.sum()))\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_grim_trigger(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        starts with cooperation but always deflects once the other player deflects a single time\n",
    "        \"\"\"\n",
    "        if sum(history_other_player) != len(history_other_player):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_random(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        random deflection / cooperation\n",
    "        \"\"\"\n",
    "        return random.random() > 0.5\n",
    "    \n",
    "    @staticmethod\n",
    "    def strategy_naive_probability(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        cooperates with same probability as the opponent \n",
    "        \"\"\"\n",
    "        if len(history_other_player) == 0:\n",
    "            return random.random() <= 0.5\n",
    "        return random.random() <= sum(history_other_player) / len(history_other_player)\n",
    "    \n",
    "    @staticmethod\n",
    "    def strategy_pavlov(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        win-stay lose-shift\n",
    "        CC (Reward, good for both) or DC (Temptation, best for this player) -> stay \n",
    "        CD (Sucker, worst for this player) or DD (Punishment, bad for both) -> shift\n",
    "        \"\"\"\n",
    "        if len(history_other_player) == 0:\n",
    "            return True\n",
    "        if history_other_player[-1]:  # CC or DC\n",
    "            return history_this_player[-1]\n",
    "        else:  # CD or DD\n",
    "            return not history_this_player[-1]\n",
    "        \n",
    "    @staticmethod\n",
    "    def strategy_pavlov_suspicious(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        like win-stay lose-shift, but starting with defecting\n",
    "        \"\"\"\n",
    "        if len(history_other_player) == 0:\n",
    "            return False\n",
    "        if history_other_player[-1]:  # CC or DC\n",
    "            return history_this_player[-1]\n",
    "        else:  # CD or DD\n",
    "            return not history_this_player[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986bf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy lookup setting:\n",
      "select c: 3, select d: 82, unseen 256\n"
     ]
    }
   ],
   "source": [
    "## setup ##\n",
    "n_runs = 10\n",
    "n_train_episodes = 15_000\n",
    "n_repetitions = 5\n",
    "# q model\n",
    "q_max_history = 4\n",
    "q_max_exploration_rate = 1.\n",
    "q_min_exploration_rate = 0.05\n",
    "q_learning_rate = 0.05\n",
    "q_discount_factor = 0.95\n",
    "q_reward_function = \"raw\"\n",
    "\n",
    "## competitor pool ##\n",
    "nonpara = NonparametricStrategies()\n",
    "competitor_pool = {\n",
    "    \"always_defect\": nonpara.strategy_always_defect,\n",
    "    \"random\": nonpara.strategy_random, \n",
    "    \"grim_trigger\": nonpara.strategy_grim_trigger, \n",
    "    \"naive_probability\": nonpara.strategy_naive_probability, \n",
    "    \"pavlov\": nonpara.strategy_pavlov, \n",
    "    \"pavlov_suspicious\": nonpara.strategy_pavlov_suspicious, \n",
    "    \"tit_for_tat\": nonpara.strategy_tit_for_tat, \n",
    "    \"tit_for_tat_suspicious\": nonpara.strategy_tit_for_tat_suspicious}\n",
    "# greedy lookup competitor\n",
    "model_greedy_lookup = QModel(max_history=q_max_history, discount_factor=0, exploration_rate=1., reward_function=q_reward_function)\n",
    "for _ in range(n_train_episodes):\n",
    "    competitor = random.choice([i for i in competitor_pool.values()])\n",
    "    repeated_prisoners_dilemma(n_repetitions=n_repetitions, strategy_player_a=model_greedy_lookup.step, strategy_player_b=competitor)\n",
    "print(\"greedy lookup setting:\")\n",
    "model_greedy_lookup.info()\n",
    "model_greedy_lookup.exploration_rate = 0.  # so it exploits its experience\n",
    "model_greedy_lookup.training_mode = False\n",
    "competitor_pool[\"greedy_lookup\"] = model_greedy_lookup.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d530af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## experiments ##\n",
    "experiment_results = {}\n",
    "for c in competitor_pool.keys():\n",
    "    experiment_results[c] = {\"q_win\": 0, \"tie\": 0, \"q_lose\": 0, \"q_reward\": 0, \"opponent_reward\": 0, \"max_possible_reward\": n_repetitions * 5 * n_runs, \"q_CC\": 0, \"q_CD\": 0, \"q_DC\": 0, \"q_DD\": 0}\n",
    "for _ in range(n_runs):\n",
    "    for competitor_name_test in [i for i in competitor_pool.keys()]:\n",
    "        q_model = QModel(max_history=q_max_history, learning_rate=q_learning_rate, discount_factor=q_discount_factor, reward_function=q_reward_function)\n",
    "        # train\n",
    "        exploration_rates = np.linspace(start=q_max_exploration_rate, stop=q_min_exploration_rate, num=n_train_episodes)\n",
    "        for i in range(n_train_episodes):\n",
    "            if random.random() <= 0.3:\n",
    "                competitor_name_train = competitor_name_test\n",
    "            else:\n",
    "                competitor_name_train = random.choice([k for k in competitor_pool.keys()])\n",
    "            competitor_train = competitor_pool[competitor_name_train]\n",
    "            # decay exploration_rate\n",
    "            q_model.exploration_rate = exploration_rates[i]\n",
    "            repeated_prisoners_dilemma(n_repetitions=n_repetitions, strategy_player_a=q_model.step, strategy_player_b=competitor_train)\n",
    "        # test\n",
    "        competitor_test = competitor_pool[competitor_name_test]\n",
    "        q_model.training_mode = False\n",
    "        scores, histories = repeated_prisoners_dilemma(n_repetitions=n_repetitions, strategy_player_a=q_model.step, strategy_player_b=competitor_test)\n",
    "        \n",
    "        if scores[0] == scores[1]:\n",
    "            experiment_results[competitor_name_test][\"tie\"] += 1\n",
    "        elif scores[0] > scores[1]:\n",
    "            experiment_results[competitor_name_test][\"q_win\"] += 1\n",
    "        else:\n",
    "            experiment_results[competitor_name_test][\"q_lose\"] += 1\n",
    "        experiment_results[competitor_name_test][\"q_reward\"] += scores[0]\n",
    "        experiment_results[competitor_name_test][\"opponent_reward\"] += scores[1]\n",
    "\n",
    "        for i in range(len(histories[0])):\n",
    "            if histories[0][i] and histories[1][i]:\n",
    "                experiment_results[competitor_name_test][\"q_CC\"] += 1\n",
    "            elif histories[0][i] and not histories[1][i]:\n",
    "                experiment_results[competitor_name_test][\"q_CD\"] += 1\n",
    "            elif not histories[0][i] and histories[1][i]:\n",
    "                experiment_results[competitor_name_test][\"q_DC\"] += 1\n",
    "            else:\n",
    "                experiment_results[competitor_name_test][\"q_DD\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ac3e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_win</th>\n",
       "      <th>tie</th>\n",
       "      <th>q_lose</th>\n",
       "      <th>q_reward</th>\n",
       "      <th>opponent_reward</th>\n",
       "      <th>max_possible_reward</th>\n",
       "      <th>q_CC</th>\n",
       "      <th>q_CD</th>\n",
       "      <th>q_DC</th>\n",
       "      <th>q_DD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>competitor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>always_defect</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "      <td>110</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>92</td>\n",
       "      <td>250</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grim_trigger</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>250</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naive_probability</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>129</td>\n",
       "      <td>250</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pavlov</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>125</td>\n",
       "      <td>250</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pavlov_suspicious</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>114</td>\n",
       "      <td>109</td>\n",
       "      <td>250</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tit_for_tat</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>250</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tit_for_tat_suspicious</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>250</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greedy_lookup</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>98</td>\n",
       "      <td>250</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        q_win  tie  q_lose  q_reward  opponent_reward  \\\n",
       "competitor                                                              \n",
       "always_defect               0    0      10        35              110   \n",
       "random                      6    1       3       132               92   \n",
       "grim_trigger                0   10       0       140              140   \n",
       "naive_probability           5    4       1       144              129   \n",
       "pavlov                      3    7       0       140              125   \n",
       "pavlov_suspicious           3    5       2       114              109   \n",
       "tit_for_tat                 0   10       0       140              140   \n",
       "tit_for_tat_suspicious      0    0      10       100              150   \n",
       "greedy_lookup               0    3       7        63               98   \n",
       "\n",
       "                        max_possible_reward  q_CC  q_CD  q_DC  q_DD  \n",
       "competitor                                                           \n",
       "always_defect                           250     0    15     0    35  \n",
       "random                                  250    13     8    16    13  \n",
       "grim_trigger                            250    30    10    10     0  \n",
       "naive_probability                       250    26    10    13     1  \n",
       "pavlov                                  250    24    10    13     3  \n",
       "pavlov_suspicious                       250    12    12    13    13  \n",
       "tit_for_tat                             250    30    10    10     0  \n",
       "tit_for_tat_suspicious                  250     3    28    18     1  \n",
       "greedy_lookup                           250    10     7     0    33  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(experiment_results, orient=\"index\")\n",
    "df.index.name = \"competitor\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2618174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = init\n",
      "select action\n",
      "Q(init, False): 5.393043761049864\n",
      "Q(init, True): 6.805050335244032\n",
      "so: True\n",
      "\n",
      "state = [1 1]\n",
      "last state = init\n",
      "update Q(s,a) = Q(init,True)\n",
      "Q(s,a): 6.805050335244032\n",
      "Q(s',a')_max_a': 10.249926339198746\n",
      "Q(s,a) <- 6.805050335244032 + 0.05 * (3 + 0.95 * 10.249926339198746 - 6.805050335244032) = 7.101669319593771\n",
      "select action\n",
      "Q([1 1], False): 7.763359816089103\n",
      "Q([1 1], True): 10.249926339198746\n",
      "so: True\n",
      "\n",
      "state = [1 1 1 1]\n",
      "last state = [1 1]\n",
      "update Q(s,a) = Q([1 1],True)\n",
      "Q(s,a): 10.249926339198746\n",
      "Q(s',a')_max_a': 7.737779529748915\n",
      "Q(s,a) <- 10.249926339198746 + 0.05 * (3 + 0.95 * 7.737779529748915 - 10.249926339198746) = 10.254974549901881\n",
      "select action\n",
      "Q([1 1 1 1], False): 6.453837375035388\n",
      "Q([1 1 1 1], True): 7.737779529748915\n",
      "so: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((9, 9), ([True, True, True], [True, True, True]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verbose train for demonstration\n",
    "q_model.verbose = True\n",
    "q_model.training_mode = True\n",
    "repeated_prisoners_dilemma(n_repetitions=3, strategy_player_a=q_model.step, strategy_player_b=competitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134d7ee",
   "metadata": {},
   "source": [
    "# QModel vs QModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce13cb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tie': 9,\n",
       " 'a_wins': 0,\n",
       " 'b_wins': 1,\n",
       " 'a_reward': 72,\n",
       " 'b_reward': 77,\n",
       " 'CC': 10,\n",
       " 'CD': 2,\n",
       " 'DC': 1,\n",
       " 'DD': 37}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_results = {\"tie\": 0, \"a_wins\": 0, \"b_wins\": 0, \"a_reward\": 0, \"b_reward\": 0, \"CC\": 0, \"CD\": 0, \"DC\": 0, \"DD\": 0}\n",
    "for i in range(n_runs):\n",
    "    q_model_a = QModel(max_history=q_max_history, learning_rate=q_learning_rate, discount_factor=q_discount_factor, reward_function=q_reward_function)\n",
    "    q_model_b = QModel(max_history=q_max_history, learning_rate=q_learning_rate, discount_factor=q_discount_factor, reward_function=q_reward_function)\n",
    "    # train\n",
    "    exploration_rates = np.linspace(start=q_max_exploration_rate, stop=q_min_exploration_rate, num=n_train_episodes)\n",
    "    for i in range(n_train_episodes):\n",
    "        # decay exploration_rate\n",
    "        q_model.exploration_rate = exploration_rates[i]\n",
    "        repeated_prisoners_dilemma(n_repetitions=n_repetitions, strategy_player_a=q_model_a.step, strategy_player_b=q_model_b.step)\n",
    "    # test\n",
    "    competitor_test = competitor_pool[competitor_name_test]\n",
    "    q_model_a.training_mode = False\n",
    "    q_model_b.training_mode = False\n",
    "    scores, histories = repeated_prisoners_dilemma(n_repetitions=n_repetitions, strategy_player_a=q_model_a.step, strategy_player_b=q_model_b.step)\n",
    "\n",
    "    if scores[0] == scores[1]:\n",
    "        experiment_results[\"tie\"] += 1\n",
    "    elif scores[0] > scores[1]:\n",
    "        experiment_results[\"a_wins\"] += 1\n",
    "    else:\n",
    "        experiment_results[\"b_wins\"] += 1\n",
    "    experiment_results[\"a_reward\"] += scores[0]\n",
    "    experiment_results[\"b_reward\"] += scores[1]\n",
    "\n",
    "    for i in range(len(histories[0])):\n",
    "        if histories[0][i] and histories[1][i]:\n",
    "            experiment_results[\"CC\"] += 1\n",
    "        elif histories[0][i] and not histories[1][i]:\n",
    "            experiment_results[\"CD\"] += 1\n",
    "        elif not histories[0][i] and histories[1][i]:\n",
    "            experiment_results[\"DC\"] += 1\n",
    "        else:\n",
    "            experiment_results[\"DD\"] += 1\n",
    "experiment_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
