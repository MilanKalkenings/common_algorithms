{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144491b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins a: 41\n",
      "wins b: 4\n",
      "ties: 55\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Callable\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def score(player_a: bool, player_b: bool) -> Tuple[int, int]:\n",
    "    if player_a and player_b:\n",
    "        return (3, 3)\n",
    "    if player_a and not player_b:\n",
    "        return (0, 5)\n",
    "    if not player_a and player_b:\n",
    "        return (5, 0)\n",
    "    else:\n",
    "        return (1, 1)\n",
    "    \n",
    "\n",
    "def repeated_prisoners_dilemma(n_repetitions: int, strategy_player_a: Callable, strategy_player_b: Callable) -> Tuple[Tuple[int, int], Tuple[List[bool], List[bool]]]:\n",
    "    score_a = 0\n",
    "    score_b = 0\n",
    "    history_player_a = []\n",
    "    history_player_b = []\n",
    "    for _ in range(n_repetitions):\n",
    "        turn_player_a = strategy_player_a(history_this_player=history_player_a, history_other_player=history_player_b)\n",
    "        turn_player_b = strategy_player_b(history_this_player=history_player_b, history_other_player=history_player_a)\n",
    "        turn_scores = score(player_a=turn_player_a, player_b=turn_player_b)\n",
    "        score_a += turn_scores[0]\n",
    "        score_b += turn_scores[1]\n",
    "        history_player_a.append(turn_player_a)\n",
    "        history_player_b.append(turn_player_b)\n",
    "    return ((score_a, score_b), (history_player_a, history_player_b))\n",
    "\n",
    "\n",
    "\n",
    "class QModel:\n",
    "    \"\"\"\n",
    "    True: player cooperates\n",
    "    False: player defects\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon: float = 0.1, max_history: int = 5, verbose: bool = False):\n",
    "        self.verbose = verbose\n",
    "        self.epsilon = epsilon\n",
    "        self.max_history = max_history\n",
    "        self.q = {\"init\": {False: [1, 1], True: [1, 1]}}\n",
    "\n",
    "\n",
    "    def select_action(self, state: str, explore: bool):\n",
    "        # exploit state knowledge\n",
    "        score_ratio_false = self.q[state][False][0] / self.q[state][False][1]\n",
    "        score_ratio_true = self.q[state][True][0] / self.q[state][True][1]  \n",
    "        selected_action = score_ratio_true >= score_ratio_false \n",
    "        if self.verbose:\n",
    "            print(\"select action\")\n",
    "            print(f\"q[state][False]: {self.q[state][False]}\")\n",
    "            print(f\"q[state][True]: {self.q[state][True]}\")\n",
    "            print(f\"so: {score_ratio_true >= score_ratio_false}\\n\")\n",
    "        # explore by deviating randomly (swaps selected action (True or False) with probability self.epsilon)\n",
    "        if explore:\n",
    "            if selected_action:\n",
    "                if random.random() <= self.epsilon:\n",
    "                    return False\n",
    "                return True\n",
    "            else:\n",
    "                if random.random() <= self.epsilon:\n",
    "                    return True\n",
    "                return False\n",
    "        else:\n",
    "            return selected_action\n",
    "    \n",
    "\n",
    "    def update_q(self, state: str, turn: bool, scores: Tuple[int, int]):\n",
    "        if state in self.q.keys():\n",
    "            if self.verbose: print(\"update state:\", f\"self.q[{state}][{turn}] = [{self.q[state][turn][0]} + {scores[0]}, {self.q[state][turn][1]} + {scores[1]}]\")\n",
    "            self.q[state][turn] = [self.q[state][turn][0] + scores[0], self.q[state][turn][1] + scores[1]]\n",
    "        else:\n",
    "            self.q[state] = {False: [1, 1], True: [1, 1]}\n",
    "            self.q[state][turn][0] = self.q[state][turn][0] + scores[0]\n",
    "            self.q[state][turn][1] = self.q[state][turn][1] + scores[1] \n",
    "            if self.verbose: print(\"new state:\", f\"self.q[{state}][{turn}] = [{self.q[state][turn][0] + scores[0]}, {self.q[state][turn][1] + scores[1]}]\")\n",
    "\n",
    "    def play_and_update(self, history_this_player: List[bool], history_other_player: List[bool]) -> bool:\n",
    "        return self._play_and_update(history_this_player=history_this_player, history_other_player=history_other_player, update_q=True)\n",
    "\n",
    "    def play(self, history_this_player: List[bool], history_other_player: List[bool]) -> bool:\n",
    "        return self._play_and_update(history_this_player=history_this_player, history_other_player=history_other_player, update_q=False)\n",
    "\n",
    "    def _play_and_update(self, history_this_player: List[bool], history_other_player: List[bool], update_q: bool) -> bool:\n",
    "        if len(history_this_player) == 0:\n",
    "            if self.verbose: print(\"first step so state = init\")\n",
    "            state = \"init\"\n",
    "        else:\n",
    "            state_np = np.concat([[int(i) for i in history_this_player[-self.max_history:][::-1]], [int(i) for i in history_other_player[-self.max_history:]]])\n",
    "            state = str(state_np)\n",
    "            if self.verbose: print(f\"state = {state}\")\n",
    "            if update_q:\n",
    "                scores_last_turn = score(player_a=history_this_player[-1], player_b=history_other_player[-1])\n",
    "                if len(history_this_player) == 1:\n",
    "                    last_state = \"init\"\n",
    "                else:\n",
    "                    last_state_np = np.concat([[int(i) for i in history_this_player[-(self.max_history+1):][::-1]], [int(i) for i in history_other_player[-(self.max_history+1):]]])\n",
    "                    last_state = str(last_state_np[1:-1])\n",
    "                if self.verbose: print(f\"last state = {last_state}\")\n",
    "                self.update_q(state=last_state, turn=history_this_player[-1], scores=scores_last_turn)\n",
    "        if state in self.q.keys():\n",
    "            return self.select_action(state=state, explore=update_q)\n",
    "        else:\n",
    "            return random.random() > 0.2\n",
    "\n",
    "\n",
    "class StatelessStrategies:\n",
    "    \"\"\"\n",
    "    True: player cooperates\n",
    "    False: player defects\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def strategy_always_cooperate(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_always_defect(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_tit_for_tat(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        start with cooperation and afterwards exactly mirrors the last turn of the opponent\n",
    "        \"\"\"\n",
    "        if len(history_this_player) == 0:\n",
    "            return True\n",
    "        return history_other_player[-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def strategy_tit_for_tat_generous(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        tit for tat but with 10% chance forgives deflection of other player in last turn\n",
    "        \"\"\"\n",
    "        if len(history_this_player) == 0:\n",
    "            return True\n",
    "        regular_turn = history_other_player[-1] \n",
    "        if regular_turn == False:\n",
    "            return random.random() > 0.9\n",
    "        return regular_turn\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_tit_for_tat_suspicious(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        tit for tat but starting with deflection (prefered in hostile environments)\n",
    "        \"\"\"\n",
    "        if len(history_this_player) == 0:\n",
    "            return False\n",
    "        return history_other_player[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_tit_for_tat_noisy(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        randomly deviates from regular tit for tat turn with 10% chance \n",
    "        \"\"\"\n",
    "        if len(history_this_player) == 0:\n",
    "            regular_turn = True\n",
    "        else:\n",
    "            regular_turn = history_other_player[-1]\n",
    "\n",
    "        if random.random() > 0.9:\n",
    "            return not regular_turn\n",
    "        return regular_turn\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_tit_for_tat_exponential_decay(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        tit for tat but considering the history of the opponents actions, assigning more weight to recent moves.\n",
    "        The player cooperates with a probability proportional to the weighted fraction of the opponents cooperation.\n",
    "        \"\"\"\n",
    "        if len(history_this_player) == 0:\n",
    "            return True\n",
    "        weights = np.logspace(start=0.1, stop=1, base=10, num=len(history_other_player))\n",
    "        return bool(random.random() < (weights[history_other_player].sum() / weights.sum()))\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_grim_trigger(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        starts with cooperation but always deflects once the other player deflects a single time\n",
    "        \"\"\"\n",
    "        if sum(history_other_player) != len(history_other_player):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def strategy_random(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        random deflection / cooperation\n",
    "        \"\"\"\n",
    "        return random.random() > 0.5\n",
    "    \n",
    "    @staticmethod\n",
    "    def strategy_naive_probability(history_this_player: List[bool], history_other_player: List[bool]):\n",
    "        \"\"\"\n",
    "        starts with cooperation, then cooperates with same probability as the opponent \n",
    "        \"\"\"\n",
    "        if len(history_other_player) == 0:\n",
    "            return True\n",
    "        return random.random() <= sum(history_other_player) / len(history_other_player)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: \n",
    "# models with large max_history fail to explore all possible states with frew training iterations \n",
    "# # -> also track partials states to fallback to knowledge about <max_history turns\n",
    "wins_a = 0\n",
    "wins_b = 0\n",
    "ties = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    model_a = QModel(epsilon=0.1, max_history=3)\n",
    "    model_b = QModel(epsilon=0.1, max_history=4)\n",
    "    \n",
    "    # train\n",
    "    for _ in range(10):\n",
    "        repeated_prisoners_dilemma(n_repetitions=20, strategy_player_a=model_a.play_and_update, strategy_player_b=model_b.play_and_update)\n",
    "\n",
    "    # test\n",
    "    model_a.epsilon = 0\n",
    "    model_b.epsilon = 0\n",
    "    scores, _ = repeated_prisoners_dilemma(n_repetitions=10, strategy_player_a=model_a.play, strategy_player_b=model_b.play)\n",
    "    if scores[0] > scores[1]:\n",
    "        wins_a += 1\n",
    "    elif scores[0] < scores[1]:\n",
    "        wins_b += 1\n",
    "    else:\n",
    "        ties += 1\n",
    "print(f\"wins a: {wins_a}\\nwins b: {wins_b}\\nties: {ties}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
